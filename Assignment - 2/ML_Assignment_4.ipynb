{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ufukdogann/Machine-Learning/blob/master/Assignment%20-%202/ML_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R1kKJk_rJC34"
      },
      "source": [
        "# Machine Learning 2019/2020: Assignment 4 -  Reinforcement Learning\n",
        "Deadline: Friday 6th of December 2019 9pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DLtJ0vlKJReh"
      },
      "source": [
        "First name: UFUK  \n",
        "Last name: DOGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u8xajf5MJ8Ud"
      },
      "source": [
        "## About this assignment\n",
        "\n",
        "In this assignment you will further deepen your understanding of Reinforcement Learning (RL).\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "Please write your answers, equations, and code directly in this python notebook and print the final result to pdf (File > Print).\n",
        "Make sure that code has appropriate line breaks such that all code is visible in the final pdf.\n",
        "Also select A3 for the PDF size to prevent content from being clipped.\n",
        "\n",
        "The final pdf must be named name.lastname.pdf and uploaded to the iCorsi website before the deadline expires. Late submissions will result in 0 points.\n",
        "\n",
        "**Also share this notebook (top right corner 'Share') with teaching.idsia@gmail.com during submission.**\n",
        "\n",
        "**Keep your answers brief and respect the sentence limits in each question (answers exceeding the limit are not taken into account)**.\n",
        "\n",
        "Learn more about python notebooks and formatting here: https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "## How to get help\n",
        "\n",
        "We encourage you to use the tutorials to ask questions or to discuss exercises with other students.\n",
        "However, do not look at any report written by others or share your report with others. Violation of that rule will result in 0 points for all students involved. For further questions you can send an email to louis@idsia.ch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w3uo_XEzLkxc"
      },
      "source": [
        "## 1 Basic probability (6p)\n",
        "\n",
        "Suppose that a migrating lizard that rests in Ticino can be in four different states:\n",
        "Eating (E), Sleeping (S), Fighting (F) and Mating (M), for example protecting its territory against other lizards. Each lizard spends 30% of its time sleeping, 40% eating, 20% fighting and the remaining time mating. A biologist collects a population of lizards and puts them in a cage to study their behaviors. Suppose the probability for a lizard being caught while eating is 0.1, for a sleeping lizard 0.4, for a fighting lizard 0.8 and for the lizards that are mating 0.2, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hRg4g3_NMXVI"
      },
      "source": [
        "### Question 1.1 (3p)\n",
        "What is the relative frequency (probability) for a lizard being caught in the cage?\n",
        "\n",
        "---\n",
        "\n",
        "$[((0.4)*(0.3))+((0.4)*(0.1)+((0.8)*(0.2))+((0.1)*(0.2))]$ = 0.34"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8OO7HpEwMdnH"
      },
      "source": [
        "### Question 1.2 (3p)\n",
        "\n",
        "What is the proportion of lizards that are fighting of those that were caught in the cage?\n",
        "\n",
        "---\n",
        "\n",
        "$[(0.2)*(0.8)] / 0.34 = 0.47$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "okJpynzHMpA6"
      },
      "source": [
        "## 2 Markov Decision Processes (32p)\n",
        "\n",
        "Suppose a robot is put in a maze with long corridor. The\n",
        "corridor is 1 kilometer long and 5 meters wide. The available actions to the robot are moving forward for 1 meter, moving backward for 1 meter, turning left for 90 degrees and turning right for 90 degrees. If the robot moves and hits the wall, then it will stay in its position and orientation. The robot's goal is to escape from this maze by reaching the end of the long corridor.\n",
        "**Note: the answers in the following questions should not exceed 5 sentences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rI3AATMANzC_"
      },
      "source": [
        "### Question 2.1 (4p)\n",
        "\n",
        "Assume the robot receives a +1 reward signal for each time step taken in the\n",
        "maze and +1000 for reaching the final goal (the end of the long corridor). Then you train the robot for a while, but it seems it still does not perform well at all for navigating to the end of the corridor in the maze. What is happening? Is there something wrong with the reward function?\n",
        "\n",
        "---\n",
        "\n",
        "Yes, because it finds a way to have more reward without reaching the goal like, 1 mt go forward and 1 mt go backwards. Robot should be rewarded only for the result, not for the actions it take."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LC-aYmiFOAEZ"
      },
      "source": [
        "### Question 2.2 (4p)\n",
        "\n",
        "If there is something wrong with the reward function, how could you fix it? If not, how to resolve the training issues?\n",
        "\n",
        "---\n",
        "\n",
        "We can also assign a penalty for each second it stays in the maze for example let's say -1, so it will try to find a way for escaping from maze as soon as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zzIDv8qoOGuH"
      },
      "source": [
        "### Questions 2.3 (2p)\n",
        "\n",
        "The discounted return for a non-episodic task is defined as\n",
        "$$\n",
        "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
        "$$\n",
        "where $\\gamma \\in [0, 1]$ is the discount factor.\n",
        "\n",
        "Rewrite the above equation such that $G_t$ is on the left hand side and $G_{t+1}$ is on the right hand side.\n",
        "\n",
        "---\n",
        "\n",
        "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$ = $R_{t+1} + \\gamma(R_{t+2} + \\gamma (R_{t+3}) + \\gamma^2(R_{t+4}) + \\ldots)$\n",
        "\n",
        "\n",
        "$(R_{t+2} + \\gamma (R_{t+3}) + \\gamma^2(R_{t+4}) + \\ldots)$ = $G_{t+1}$\n",
        "\n",
        "$G_t$ = $R_{t+1} + \\gamma G_{t+1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HTxEimD5O5eA"
      },
      "source": [
        "### Questions 2.4 (2p)\n",
        "\n",
        "What is the sufficient condition for this infinite series to be a convergent series?\n",
        "\n",
        "---\n",
        "\n",
        "$\\gamma$ should be less than 1 and assuming that the sequence of the reward is bounded.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bp30j2csPJkz"
      },
      "source": [
        "### Questions 2.5 (5p)\n",
        "\n",
        "Suppose this infinite series is a convergent series, and each reward in the series is a constant of +1. We know the series is bounded, what is a simple formula for this bound ? Write it down without using summation.\n",
        "\n",
        "---\n",
        "\n",
        "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$ \n",
        "\n",
        "$G_t = 1 + \\gamma + \\gamma^2 + \\ldots$ \n",
        "\n",
        "$G_t =  1 / 1 - \\gamma$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bmDQKWHNPSnx"
      },
      "source": [
        "### Questions 2.6 (5p)\n",
        "\n",
        "Let the task be an episodic setting and the robot is running for $T = 5$ time steps. Suppose $\\gamma = 0.3$, and the robot receives rewards along the way $R_1 = −1, R_2 = −0.5, R_3 = 2, R_4 = 1, R_5 = 6$. What are the values for $G_0, G_1, G_2, G_3, G_4, G_5$?\n",
        "\n",
        "---\n",
        "\n",
        "So, the main equation is this:\n",
        "\n",
        "$G_t$ = $R_{t+1} + \\gamma G_{t+1}$\n",
        "\n",
        "Using main equation, we can write other equations as well.\n",
        "\n",
        "$\\begin{array}{ll}{G_{0}=R_{1}+\\gamma * G_{1},} & {\\text { where } t<T} \\\\ {G_{1}=R_{2}+\\gamma * G_{2},} & {\\text { where } t<T} \\\\ {G_{2}=R_{3}+\\gamma * G_{3},} & {\\text { where } t<T} \\\\ {G_{3}=R_{4}+\\gamma * G_{4},} & {\\text { where } t<T} \\\\ {G_{4}=R_{5}+\\gamma * G_{5},} & {\\text { where } t<T} \\\\ {G_{5}=R_{6}+\\gamma * G_{6},} & {\\text { where } t<T}\\end{array}$\n",
        "\n",
        "If $t_{1}$ > T we have to assign $G_{t}$ = 0. So, that's why $G_{5}$ is 0, then we can apply the equations above, the results would be;\n",
        "\n",
        "\n",
        "$G_{5} = 0$\n",
        "\n",
        "$G_{4} = 6$\n",
        "\n",
        "$G_{3} = 2.8$\n",
        "\n",
        "$G_{2} = 2.84$\n",
        "\n",
        "$G_{1} = 0.35$\n",
        "\n",
        "$G_{0} = -0.894$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G7ZcAEvfQA8f"
      },
      "source": [
        "### Questions 2.7 (5p)\n",
        "\n",
        "Suppose each reward in the series is increased by a constant $c$, i.e. $R_t \\leftarrow R_t + c$.\n",
        "Then how does it change $G_t$?\n",
        "\n",
        "---\n",
        "$\\begin{array}{l}{\\text { The equation is } G_{t}=\\sum_{k=0}^{\\infty} \\gamma^{k} * R_{t+k+1}} \\\\ {\\text { In this equation, if we change the reward to } R_{t+k+1}+c \\text { then the new equation will be }} \\\\ {\\sum_{k=0}^{\\infty} \\gamma^{k} *\\left(R_{t+k+1}\\right)+\\left(\\gamma^{k} * c\\right)}\\end{array}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j7KPVdhPQBpz"
      },
      "source": [
        "### Questions 2.8 (5p)\n",
        "\n",
        "Now consider episodic tasks, and similar to Question 2.7, we add a constant $c$ to each reward, how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "$\\begin{array}{l}{\\text { The equation is } G_{t}=\\sum_{k=t+1}^{T} \\gamma^{k-t-1} * R_{k} .} \\\\ {\\text { In this equation, if we change the reward to } R_{k}+c \\text { then the new equation will be }} \\\\ {\\sum_{k=t+1}^{T} \\gamma^{k-t-1} *\\left(R_{k}\\right)+\\left(\\gamma^{k-t-1} * c\\right)}\\end{array}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kjOnM7AVQVOB"
      },
      "source": [
        "## 3 Dynamic Programming (62p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Jm0EyIHQaOp"
      },
      "source": [
        "### Questions 3.1 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state value function without using expectation notation, but using probability distributions instead. \n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "$\\begin{aligned} v_{*}(s) &=\\max _{a \\in \\mathcal{A}(s)} q_{\\pi_{*}}(s, a) \\\\ &=\\max _{a} \\mathbb{E}_{\\pi_{*}}\\left[G_{t} | S_{t}=s, A_{t}=a\\right] \\\\ &=\\max _{a} \\mathbb{E}_{\\pi_{*}}\\left[R_{t+1}+\\gamma G_{t+1} | S_{t}=s, A_{t}=a\\right] \\\\ &=\\max _{a} \\mathbb{E}\\left[R_{t+1}+\\gamma v_{*}\\left(S_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ &=\\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{*}\\left(s^{\\prime}\\right)\\right] \\\\ &=\\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{*}\\left(s^{\\prime}\\right)\\right] \\end{aligned}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f0VZcK6LQkUC"
      },
      "source": [
        "### Questions 3.2 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state-action value function without using expectation notation, but using probability distributions instead.\n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "$\\begin{aligned} q_{*}(s, a) &=\\mathbb{E}\\left[R_{t+1}+\\gamma \\max _{a^{\\prime}} q_{*}\\left(S_{t+1}, a^{\\prime}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ &=\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma \\max _{a^{\\prime}} q_{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\right] \\end{aligned}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tq66sRJeQlE2"
      },
      "source": [
        "### Questions 3.3 (15p)\n",
        "\n",
        "Consider a 4x4 gridworld depicted in the following table:\n",
        "\n",
        "![Grid world](https://i.ibb.co/HdSdKJB/image.png)\n",
        "\n",
        "The non-terminal states are $S = \\{1, 2, \\ldots, 14\\}$ and the terminal states are $\\bar S = \\{0, 15\\}$.\n",
        "There are four available actions for each state, that is $A = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\}$.\n",
        "Assume the state transitions are deterministic and all transitions result in a negative reward −1 (after termination all rewards are zero).\n",
        "If the agent hits the boundary, then its state will remain unchanged, e.g. $p(s=8, r=−1|s=8, a=\\text{left}) = 1$.\n",
        "Note: In this exercise, we assume the policy is a deterministic\n",
        "function.\n",
        "\n",
        "Manually run the policy iteration algorithm (see lecture slide 58) for one iteration. Use the in-place policy iteration algorithm.\n",
        "This means one time policy evaluation with a single pass through the states (16 equations) and one time policy improvement.\n",
        "Assume the initial state value for all 16 cells are 0.0 and the policy initially always outputs the 'left' action.\n",
        "Write down the equations and detailed numerical computations for the updated values of each cell.\n",
        "Use a discount factor $\\gamma = 0.5$.\n",
        "Write down the policy after policy improvement.\n",
        "\n",
        "![Policy iteration](http://www.incompleteideas.net/book/ebook/imgtmp5.png)\n",
        "\n",
        "Read more about this in Sutton & Barto's book http://www.incompleteideas.net/book/ebook/node43.html\n",
        "\n",
        "---\n",
        "\n",
        "### Policy Evaulation\n",
        "\n",
        "$v_{k+1}(s)=\\sum_{a} \\pi(a | s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right]$\n",
        "\n",
        "Using the above equation I found the polivy evaulation values like below;\n",
        "\n",
        "$v_{1}(0)=(0+0.5 \\cdot 0)=0$\n",
        "\n",
        "$v_{1}(1)=(-1+0.5 \\cdot 0)=-1$\n",
        "\n",
        "$v_{1}(2)=(-1+0.5 \\cdot -1)=-1.5$\n",
        "\n",
        "$v_{1}(3)=(-1+0.5 \\cdot -1.5)=-1.75$\n",
        "\n",
        "$v_{1}(4)=(-1+0.5 \\cdot 0)=-1$\n",
        "\n",
        "$v_{1}(5)=(-1+0.5 \\cdot -1)=-1.5$\n",
        "\n",
        "$v_{1}(6)=(-1+0.5 \\cdot -1.5)=-1.75$\n",
        "\n",
        "$v_{1}(7)=(-1+0.5 \\cdot -1.75)=-1.87$\n",
        "\n",
        "$v_{1}(8)=(-1+0.5 \\cdot 0)=-1$\n",
        "\n",
        "$v_{1}(9)=(-1+0.5 \\cdot -1)=-1.5$\n",
        "\n",
        "$v_{1}(10)=(-1+0.5 \\cdot -1.5)=-1.75$\n",
        "\n",
        "$v_{1}(11)=(-1+0.5 \\cdot -1.75)=-1.87$\n",
        "\n",
        "$v_{1}(12)=(-1+0.5 \\cdot 0)=-1$\n",
        "\n",
        "$v_{1}(13)=(-1+0.5 \\cdot -1)=-1.5$\n",
        "\n",
        "$v_{1}(14)=(-1+0.5 \\cdot -1.5)=1.75$\n",
        "\n",
        "$v_{1}(15)=(0+0.5 \\cdot 0)=0$\n",
        "\n",
        "### Policy Improvement\n",
        "\n",
        "$\\pi^{\\prime}(s)=\\quad\\underset{a}{\\arg \\max } \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]$\n",
        "\n",
        "Policy for $\\textbf{state 0}$ won't change because it is terminal state and transitions only itself.\n",
        "\n",
        "$\\pi(1) = p(0,-1|1,left)[(-1)+(0.5*0)] = -1$\n",
        "\n",
        "$\\pi(1) = p(2,-1|1,right)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(1) = p(5,-1|1,down)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(1) = p(1,-1|1,up)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "So, the new policy for $\\textbf{state 1}$ would be going $\\textbf{LEFT}$\n",
        "\n",
        "$\\pi(2) = p(1,-1|2,left)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(2) = p(3,-1|2,right)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(2) = p(6,-1|2,down)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(2) = p(2,-1|2,up)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "So, the new policy for $\\textbf{state 2}$ would be going $\\textbf{LEFT}$\n",
        "\n",
        "$\\pi(3) = p(2,-1|3,left)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(3) = p(3,-1|3,right)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(3) = p(7,-1|3,down)[(-1)+(0.5*-1.87)] = -1.935$\n",
        "\n",
        "$\\pi(3) = p(3,-1|3,up)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "So, the new policy for $\\textbf{state 3}$ would be going $\\textbf{LEFT}$\n",
        "\n",
        "$\\pi(4) = p(4,-1|4,left)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(4) = p(5,-1|4,right)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(4) = p(8,-1|4,down)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(4) = p(0,-1|4,up)[(-1)+(0.5*0)] = -1$\n",
        "\n",
        "So, the new policy for $\\textbf{state 4}$ would be going $\\textbf{UP}$\n",
        "\n",
        "$\\pi(5) = p(4,-1|5,left)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(5) = p(6,-1|5,right)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(5) = p(9,-1|5,down)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(5) = p(1,-1|5,up)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "So, the new policy for $\\textbf{state 5}$ would be going $\\textbf{UP}$\n",
        "\n",
        "$\\pi(6) = p(5,-1|6,left)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(6) = p(7,-1|6,right)[(-1)+(0.5*-1.87)] = -1.935$\n",
        "\n",
        "$\\pi(6) = p(10,-1|6,down)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(6) = p(2,-1|6,up)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "So, the new policy for $\\textbf{state 6}$ would be going $\\textbf{UP}$\n",
        "\n",
        "$\\pi(7) = p(6,-1|7,left)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(7) = p(7,-1|7,right)[(-1)+(0.5*-1.87)] = -1.935$\n",
        "\n",
        "$\\pi(7) = p(11,-1|7,down)[(-1)+(0.5*-1.87)] = -1.935$\n",
        "\n",
        "$\\pi(7) = p(3,-1|7,up)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "So, the new policy for $\\textbf{state 7}$ would be going $\\textbf{UP}$\n",
        "\n",
        "$\\pi(8) = p(8,-1|8,left)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(8) = p(9,-1|8,right)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(8) = p(12,-1|8,down)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(8) = p(4,-1|8,up)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "So, the new policy for $\\textbf{state 8}$ would be going $\\textbf{UP}$\n",
        "\n",
        "$\\pi(9) = p(8,-1|9,left)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(9) = p(10,-1|9,right)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(9) = p(13,-1|9,down)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(9) = p(5,-1|9,up)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "So, the new policy for $\\textbf{state 9}$ would be going $\\textbf{LEFT}$\n",
        "\n",
        "$\\pi(10) = p(9,-1|10,left)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(10) = p(11,-1|10,right)[(-1)+(0.5*-1.87)] = -1.935$\n",
        "\n",
        "$\\pi(10) = p(14,-1|10,down)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(10) = p(6,-1|10,up)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "So, the new policy for $\\textbf{state 10}$ would be going $\\textbf{LEFT}$\n",
        "\n",
        "$\\pi(11) = p(10,-1|11,left)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(11) = p(11,-1|11,right)[(-1)+(0.5*-1.87)] = -1.935$\n",
        "\n",
        "$\\pi(11) = p(15,-1|11,down)[(-1)+(0.5*0)] = -1$\n",
        "\n",
        "$\\pi(11) = p(7,-1|11,up)[(-1)+(0.5*-1.87)] = -1.935$\n",
        "\n",
        "So, the new policy for $\\textbf{state 11}$ would be going $\\textbf{DOWN}$\n",
        "\n",
        "$\\pi(12) = p(12,-1|12,left)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(12) = p(13,-1|12,right)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(12) = p(8,-1|12,down)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(12) = p(12,-1|12,up)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "So, the new policy for $\\textbf{state 12}$ would be going $\\textbf{DOWN}$\n",
        "\n",
        "$\\pi(13) = p(12,-1|13,left)[(-1)+(0.5*-1)] = -1.5$\n",
        "\n",
        "$\\pi(13) = p(14,-1|13,right)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(13) = p(13,-1|13,down)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "$\\pi(13) = p(9,-1|13,up)[(-1)+(0.5*-1.5)] = -1.75$\n",
        "\n",
        "So, the new policy for $\\textbf{state 13}$ would be going $\\textbf{LEFT}$\n",
        "\n",
        "$\\pi(14) = p(13,-1|14,left)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(14) = p(15,-1|14,right)[(-1)+(0.5*0)] = -1$\n",
        "\n",
        "$\\pi(14) = p(14,-1|14,down)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "$\\pi(14) = p(10,-1|14,up)[(-1)+(0.5*-1.75)] = -1.875$\n",
        "\n",
        "So, the new policy for $\\textbf{state 14}$ would be going $\\textbf{RIGHT}$\n",
        "\n",
        "Policy for $\\textbf{state 15}$ won't change because it is terminal state and transitions only itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bq5MSgvnhTF",
        "colab_type": "text"
      },
      "source": [
        "#### PLEASE RUN THE CODE FOR SEEING AS AN IMAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJU2dg2OnhTG",
        "colab_type": "code",
        "colab": {},
        "outputId": "baf1c889-9cd2-4d90-edb7-b96dc7d2a2b4"
      },
      "source": [
        "policy = [0,2,2,2,0,0,0,0,0,2,2,1,1,2,3,1]\n",
        "P = np.array(policy).reshape(4, 4)\n",
        "actions_repr = np.array(['↑', '↓', '←', '→'])\n",
        "actions_repr[P]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['↑', '←', '←', '←'],\n",
              "       ['↑', '↑', '↑', '↑'],\n",
              "       ['↑', '←', '←', '↓'],\n",
              "       ['↓', '←', '→', '↓']], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ciourK1wQo-3"
      },
      "source": [
        "### Questions 3.4 (15p)\n",
        "\n",
        "Implement the beforementioned environment in the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-3gmuwlmdyO4"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkzicBh-I3dU",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=180)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DT_bNpgqd2mM"
      },
      "source": [
        "#### Defining the problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNBoBp3PJC0C",
        "colab": {}
      },
      "source": [
        "class GridWorld:\n",
        "  UP = 0\n",
        "  DOWN = 1\n",
        "  LEFT = 2\n",
        "  RIGHT = 3\n",
        "\n",
        "  def __init__(self, side=4):\n",
        "    self.side = side\n",
        "    # -------------------------\n",
        "    # Define integer states, actions, and final states as specified in the problem description\n",
        "\n",
        "    # TODO insert code here\n",
        "    self.actions = np.array([0,1,2,3])\n",
        "    self.states = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
        "    self.finals = np.array([0,15])\n",
        "\n",
        "    # -------------------------\n",
        "    self.actions_repr = np.array(['↑', '↓', '←', '→'])\n",
        "\n",
        "  def reward(self, s, s_next, a):\n",
        "    # -------------------------\n",
        "    # Return the reward for the given transition as specified in the problem description\n",
        "\n",
        "    # TODO insert code here\n",
        "    #assert(0 <= s < 16)\n",
        "        \n",
        "    if s in self.finals:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "        \n",
        "    # -------------------------\n",
        "\n",
        "  def transition_prob(self, s, s_next, a):\n",
        "    # -------------------------\n",
        "    # Return a probability in [0, 1] for the given transition as specified in the problem description\n",
        "\n",
        "    # TODO insert code here\n",
        "    \n",
        "    #UP = 0\n",
        "    #DOWN = 1\n",
        "    #LEFT = 2\n",
        "    #RIGHT = 3\n",
        "            \n",
        "        \n",
        "    if s in world.finals:\n",
        "        if s == s_next:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    if a == 0:\n",
        "        if (s - s_next == 4) or (s//4 == 0 and s_next == s):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    elif a == 1:\n",
        "        if (s_next - s == 4) or (s//4 == 3 and s_next == s):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    elif a == 2:\n",
        "        if (s%self.side == 0 and s_next == s) or (s - s_next == 1):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    elif a == 3:\n",
        "        if (s%self.side == 3 and s_next == s) or (s_next - s == 1):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    else:\n",
        "        print('Action is not valid')\n",
        "        \n",
        "    # -------------------------\n",
        "\n",
        "  def print_policy(self, policy):\n",
        "    P = np.array(policy).reshape(self.side, self.side)\n",
        "    print(self.actions_repr[P])\n",
        "  \n",
        "  def print_values(self, values):\n",
        "    V = np.array(values).reshape(self.side, self.side)\n",
        "    print(V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FqMt-0yAuGz8"
      },
      "source": [
        "### Questions 3.5 (17p)\n",
        "\n",
        "Implement policy iteration in the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source.\n",
        "\n",
        "Run the code multiple times. Do you always end up with the same policy? Why? (max 4 sentences)\n",
        "\n",
        "---\n",
        "Yes, I ended up with the same policy and the result of this is because of state transitions are deterministic. At the end of the calculations we can see that, it converges to the same policy since the state transitions are deterministic, the result of the policy evaulation values and also, policy improvement results are deterministic as well and so, our final result is deterministic as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PrUMxh-qd5u0"
      },
      "source": [
        "#### Policy iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m1DOXcH5J0NR",
        "colab": {}
      },
      "source": [
        "def eval_policy(world, policy, values, gamma=0.9, theta=0.01):\n",
        "  # --------------------------\n",
        "  # Implement policy evaluation and return the updated value function\n",
        "\n",
        "  # TODO insert code here\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        \n",
        "        for i in world.states:\n",
        "            v = values[i].copy()\n",
        "            value = 0\n",
        "            for a in world.actions:\n",
        "                if a == policy[i]:\n",
        "                    for j in world.states:\n",
        "                        transition_prob_of_i = world.transition_prob(i, j, policy[i])\n",
        "                        if transition_prob_of_i == 1:\n",
        "                            reward_of_i = world.reward(i, j, policy[i])\n",
        "                            value = transition_prob_of_i * (reward_of_i + (gamma * values[j]))\n",
        "                            #print('From ' + str(i) + ' To ' \n",
        "                            #+ str(j) + ' Prob ' + str(transition_prob_of_i) \n",
        "                            #+ ' Reward ' + str(reward_of_i) + ' Value ' + str(value))\n",
        "            values[i] = value\n",
        "            delta = max(delta, abs(v - values[i]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return values\n",
        "            \n",
        "            \n",
        "            \n",
        "#            for j in world.states:\n",
        "#                print('i',i)\n",
        "#                print('j',j)\n",
        "#                transition_prob_of_i = world.transition_prob(i, j, policy[i])\n",
        "#                print('policy', policy[i])\n",
        "#                print('transition prob',transition_prob_of_i)\n",
        "#                reward_of_i = world.reward(i, j, policy[i])\n",
        "#                print('reward',reward_of_i)\n",
        "#                value += transition_prob_of_i * (reward_of_i + (gamma * values[j]))\n",
        "#                print('value', value)\n",
        "                \n",
        "#            values[i] = value\n",
        "#            delta = max(delta, abs(v - values[i]))\n",
        "\n",
        "#        if delta < theta:\n",
        "#            break\n",
        "#    return values\n",
        "        \n",
        "  # --------------------------\n",
        "\n",
        "\n",
        "def improve_policy(world, policy, values, gamma=0.9):\n",
        "  # --------------------------\n",
        "  # Implement policy improvement and return the updated policy\n",
        "  # TODO insert code here\n",
        "\n",
        "    policy_stable = True\n",
        "\n",
        "    for i in world.states:\n",
        "        max_policy = None\n",
        "       \n",
        "        p_vals = []\n",
        "        for a in world.actions:\n",
        "            p_val = 0\n",
        "            for j in world.states:\n",
        "                transition_prob_of_i = world.transition_prob(i, j, a)\n",
        "                reward_of_i = world.reward(i, j, a)\n",
        "                p_val += transition_prob_of_i * (reward_of_i + (gamma * values[j]))\n",
        "            p_vals.append(p_val)\n",
        "                \n",
        "        max_policy = np.argmax(p_vals)\n",
        "\n",
        "        if policy[i] != max_policy:\n",
        "            policy_stable = False\n",
        "    \n",
        "        policy[i] = max_policy\n",
        "            \n",
        "    return policy_stable\n",
        "\n",
        "  # --------------------------\n",
        "\n",
        "def policy_iteration(world, gamma=0.9, theta=0.01):\n",
        "  # Initialize a random policy\n",
        "  policy = np.array([np.random.choice(world.actions) for s in world.states])\n",
        "  print('Initial policy')\n",
        "  world.print_policy(policy)\n",
        "  # Initialize values to zero\n",
        "  values = np.zeros_like(world.states, dtype=np.float32)\n",
        "\n",
        "  # Run policy iteration\n",
        "  stable = False\n",
        "  for i in itertools.count():\n",
        "    print(f'Iteration {i}')\n",
        "    values = eval_policy(world, policy, values, gamma, theta)\n",
        "    world.print_values(values)\n",
        "    stable = improve_policy(world, policy, values, gamma)\n",
        "    world.print_policy(policy)\n",
        "    if stable:\n",
        "      break\n",
        "\n",
        "  return policy, values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QqdoNw97mcEA",
        "scrolled": false,
        "colab": {},
        "outputId": "7c164620-5b70-4d86-846e-1c7d087356b9"
      },
      "source": [
        "world = GridWorld()\n",
        "policy, values = policy_iteration(world, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial policy\n",
            "[['↑' '↑' '→' '↑']\n",
            " ['←' '↓' '↓' '↓']\n",
            " ['↓' '→' '←' '↑']\n",
            " ['←' '←' '←' '↑']]\n",
            "Iteration 0\n",
            "[[ 0.    -1.992 -1.992 -1.992]\n",
            " [-1.992 -2.    -2.    -2.   ]\n",
            " [-1.992 -2.    -2.    -2.   ]\n",
            " [-1.992 -1.996 -1.998  0.   ]]\n",
            "[['↑' '←' '↑' '↑']\n",
            " ['↑' '↑' '↑' '↑']\n",
            " ['↑' '←' '↓' '↓']\n",
            " ['↑' '←' '→' '↑']]\n",
            "Iteration 1\n",
            "[[ 0.    -1.    -1.999 -1.999]\n",
            " [-1.    -1.5   -2.    -2.   ]\n",
            " [-1.5   -1.75  -1.5   -1.   ]\n",
            " [-1.75  -1.875 -1.     0.   ]]\n",
            "[['↑' '←' '←' '↑']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↑']]\n",
            "Iteration 2\n",
            "[[ 0.   -1.   -1.5  -2.  ]\n",
            " [-1.   -1.5  -1.75 -1.5 ]\n",
            " [-1.5  -1.75 -1.5  -1.  ]\n",
            " [-1.75 -1.5  -1.    0.  ]]\n",
            "[['↑' '←' '←' '↓']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↑']]\n",
            "Iteration 3\n",
            "[[ 0.   -1.   -1.5  -1.75]\n",
            " [-1.   -1.5  -1.75 -1.5 ]\n",
            " [-1.5  -1.75 -1.5  -1.  ]\n",
            " [-1.75 -1.5  -1.    0.  ]]\n",
            "[['↑' '←' '←' '↓']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↑']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hRFm6Zm6YI1"
      },
      "source": [
        "### Questions 3.6 (5p)\n",
        "\n",
        "Let's run policy iteration with $\\gamma = 1$. Describe what is happening. Why is this the case? Give an example. What is $\\gamma$ trading off and how does it affect policy iteration? (max 8 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "$\\gamma$ is called discount rate, which helps us to understand which reward our model cares about. If $\\gamma$ is closer to one, then we can say that our model is caring more to have the long term rewards than short term rewards, on the other hand, if our $\\gamma$ is equal to zero we can say that model is concern about only maximizing the immediate (short - term) rewards.\n",
        "\n",
        "If $\\gamma$ the general equation which stated below, would converge a number as long as the reward sequences are connected.\n",
        "\n",
        "$G_{t} = R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\cdots=\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$\n",
        "\n",
        "$\\textbf{It does not convergence because algorithm never reach a delta value which is smaller than theta.}$\n",
        "\n",
        "And our reward is constant for non-terminal states and greater than zero so, equation has finite result if reward is non-zero and $\\gamma$ is smaller thane 1, otherwise, geometric series will never converge.\n",
        "\n",
        "As asked in question 2.5, expected return would be undefined since we will divide 1/0. \n",
        "\n",
        "$G_t =  1 / 1 - \\gamma$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5lEbNn0nhTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}